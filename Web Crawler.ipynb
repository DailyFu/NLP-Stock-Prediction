{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer, BeautifulStoneSoup\n",
    "import datetime\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "import quandl\n",
    "import config\n",
    "import dateutil.relativedelta\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. Define Functions <br>\n",
    "    a. Get list of SEC docs for CIK<br>\n",
    "    b. Extract text, date from each link<br>\n",
    "    c. Get price given ticker, date from Quandl\n",
    "    d. Get movement given ticker, date\n",
    "    e. Get index movement\n",
    "    f. Check if date is a weekday, and if necessary, adjust to Friday before\n",
    "    g. Calculate dates for month before, quarter before, year before for eent mvoement calculations\n",
    "2. Download 8Ks\n",
    "3. Download Stock Movements\n",
    "### Process\n",
    "1. For each ticker in S&P 500:<br>\n",
    "    a. Get list of links to all 8Ks for a CIK<br>\n",
    "    b. Extract and clean up corpus<br>\n",
    "    c. Extract date and time of release<br>\n",
    "2. Stock Movements<br>\n",
    "    a. Calculate 1 Day movement(before and after release)<br>\n",
    "    b. Calculate 1 Month, 1 Quarter, 1 Year moving averages<br>\n",
    "    c. Get VIX at publication release<br>\n",
    "3. NLP\n",
    "\n",
    "### Get Corpus Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Dataframe of document links for a given CIK\n",
    "def get_sec_docs(cik=\"0001065088\"):\n",
    "    base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "    inputted_cik = cik\n",
    "    payload = {\n",
    "        \"action\" : \"getcompany\",\n",
    "        \"CIK\" : inputted_cik,\n",
    "        \"type\" : \"8-K\",\n",
    "        \"output\":\"xml\"\n",
    "        #\"dateb\" : \"20180331\",\n",
    "        #\"count\" : \"100\",\n",
    "        #\"owner\" : \"include\"\n",
    "    }\n",
    "    sec_response = requests.get(url=base_url,params=payload)\n",
    "    soup = BeautifulSoup(sec_response.text,'lxml')\n",
    "    url_list = soup.findAll('filinghref')\n",
    "    html_list = []\n",
    "    # Get html version of links\n",
    "    for link in url_list:\n",
    "        link = link.string\n",
    "        if link.split(\".\")[len(link.split(\".\"))-1] == 'htm':\n",
    "            txtlink = link + \"l\"\n",
    "            html_list.append(txtlink)\n",
    "\n",
    "    doc_list = []\n",
    "    doc_name_list = []\n",
    "    # Get links for txt versions of files\n",
    "    for k in range(len(html_list)):\n",
    "        txt_doc = html_list[k].replace(\"-index.html\",\".txt\")\n",
    "        doc_name = txt_doc.split(\"/\")[-1]\n",
    "        doc_list.append(txt_doc)\n",
    "        doc_name_list.append(doc_name)\n",
    "        # Create dataframe of CIK, doc name, and txt link\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "        \"cik\" : [cik]*len(html_list),\n",
    "        \"txt_link\" : doc_list,\n",
    "        \"doc_name\": doc_name_list\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Extracts text and submission datetime from document link\n",
    "def extract_text(link):\n",
    "    r = requests.get(link)\n",
    "    #Parse 8-K document\n",
    "    filing = BeautifulSoup(r.content,\"html.parser\",from_encoding=\"ascii\")\n",
    "    #Extract datetime\n",
    "    submission_dt = filing.find(\"acceptance-datetime\").string[:14]\n",
    "    #Extract HTML sections\n",
    "    submission_dt = datetime.datetime.strptime(submission_dt,\"%Y%m%d%H%M%S\")\n",
    "    for section in filing.findAll(\"html\"):\n",
    "        #Remove tables\n",
    "        for table in section(\"table\"):\n",
    "            table.decompose()\n",
    "        #Convert to unicode\n",
    "        section = unicodedata.normalize(\"NFKD\",section.text)\n",
    "        section = section.replace(\"\\t\",\" \").replace(\"\\n\",\" \").replace(\"/s\",\" \").replace(\"\\'\",\"'\")\n",
    "    filing = \"\".join((section))\n",
    "    \n",
    "    return filing, submission_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Stock & Index Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S&P 500 index data downloaded from Yahoo Finance GSPC\n",
    "gspc_df = pd.read_csv(\"Data/gspc.csv\",parse_dates=['Date'],index_col=\"Date\")\n",
    "\n",
    "#Authenticate with API KEY\n",
    "quandl.ApiConfig.api_key = config.api_key # YOUR API KEY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes datetime object and ticker string, returns price (opening or closing)\n",
    "def get_quandl_data(ticker,end_date,market_open=False):\n",
    "    if market_open == True:\n",
    "        quandl_param = \"WIKI/\" + ticker + \".8\"  \n",
    "    else:\n",
    "        quandl_param = \"WIKI/\" + ticker + \".11\" \n",
    "   \n",
    "    end_date = datetime.datetime.strftime(end_date,\"%Y-%m-%d\") \n",
    "    start_date = datetime.datetime.strftime(end_date,\"%Y-%m-%d\")\n",
    "    price = quandl.get(quandl_param,start_date=start_date,end_date=end_date).values[0,0]\n",
    "    return price\n",
    "\n",
    "def get_start_date(period,release_date):\n",
    "   #1 Week\n",
    "    if period == \"week\":\n",
    "        start_date = release_date + datetime.timedelta(weeks=-1)\n",
    "     #1 Month    \n",
    "    elif period == \"month\":\n",
    "        start_date = release_date + dateutil.relativedelta.relativedelta(months=-1)\n",
    "    #1 Quarter\n",
    "    elif period == \"quarter\":\n",
    "        start_date = release_date + dateutil.relativedelta.relativedelta(months=-3)\n",
    "    #1 Year\n",
    "    elif period == \"year\":\n",
    "        start_date = release_date +  dateutil.relativedelta.relativedelta(years=-1)\n",
    "    else:\n",
    "        raise KeyError\n",
    "        \n",
    "    #Check if date falls on a weekend\n",
    "    start_date = weekday_check(start_date)\n",
    "\n",
    "    return start_date\n",
    "\n",
    "def get_recent_movements(ticker,release_date):\n",
    "    #Get year of data\n",
    "    end_date = release_date + datetime.timedelta(days=-1)\n",
    "    end_date = datetime.datetime.strftime(end_date,\"%Y-%m-%d\")\n",
    "    year_before = get_start_date(release_date,\"year\")\n",
    "    year_before = datetime.datetime.strftime(year_before,\"%Y-%m-%d\")\n",
    "    params = quandl_param = \"WIKI/\" + ticker + \".11\"\n",
    "    year_df = quandl.get(quandl_param,start_date=start_date,end_date=year_before)\n",
    "    #From year, get quarterly, monthly, weekly prices\n",
    "    \n",
    "# Takes ticker, 8K release date, checks time of release and then calculate before and after price change\n",
    "def get_change(ticker,release_date):\n",
    "    market_close = release_date.replace(hour=16,minute=0,second=0)\n",
    "    market_open = release_date.replace(hour=9,minute=30,second=0)\n",
    "    \n",
    "# If report is released after market hours, take change of start date close and release date open\n",
    "    if release_date > market_close:\n",
    "        start_date = release_date\n",
    "        end_date = release_date + datetime.timedelta(days=1)\n",
    "        end_date = weekday_check(end_date)\n",
    "\n",
    "        price_before_release = get_quandl_data(ticker,start_date,market_open=False)\n",
    "        price_after_release = get_quandl_data(ticker,end_date,market_open=True)\n",
    "\n",
    "        index_before_release = get_index_price(start_date,market_open=False)\n",
    "        index_after_release = get_index_price(end_date,market_open=True)\n",
    "\n",
    "    # If report is released before market hours, take change of start date's close and release date's open\n",
    "    elif release_date < market_open:\n",
    "        start_date = release_date + datetime.timedelta(days=-1)\n",
    "        start_date = weekday_check(start_date)\n",
    "        end_date = release_date\n",
    "\n",
    "        price_before_release = get_quandl_data(ticker,start_date,market_open=False)\n",
    "        price_after_release = get_quandl_data(ticker,end_date,market_open=True) \n",
    "\n",
    "        index_before_release = get_index_price(start_date,market_open=False)\n",
    "        index_after_release = get_index_price(end_date,market_open=True)\n",
    "\n",
    "    # If report is released during market hours, use market close\n",
    "    else:\n",
    "        start_date = release_date\n",
    "        end_date = release_date\n",
    "        price_before_release = get_quandl_data(ticker,start_date,market_open=True)\n",
    "        price_after_release = get_quandl_data(ticker,end_date,market_open=False)\n",
    "\n",
    "        index_before_release = get_index_price(start_date,market_open=False)\n",
    "        index_after_release = get_index_price(end_date,market_open=False)\n",
    "       \n",
    "    price_pct_change = calculate_pct_change(price_after_release,price_before_release)\n",
    "    index_pct_change = calculate_pct_change(index_after_release,index_before_release)\n",
    "    \n",
    "    return price_pct_change - index_pct_change\n",
    "\n",
    "def get_index_price(input_date,market_open):\n",
    "    if market_open == True:\n",
    "        price = gspc_df.loc[gspc_df.index==np.datetime64(input_date.date()),\"Open\"]  \n",
    "    else:\n",
    "        price = gspc_df.loc[gspc_df.index==np.datetime64(input_date.date()),\"Adj Close\"] \n",
    "    return price\n",
    "\n",
    "def calculate_pct_change(end_value,start_value):\n",
    "    pct_change = (end_value - start_value) / start_value\n",
    "    pct_change = round(pct_change,4) * 100\n",
    "    return pct_change\n",
    "\n",
    "def weekday_check(date):\n",
    "    # If date is Saturday or Sunday, reset date to the preceding Friday\n",
    "    if date.isoweekday() == 6:\n",
    "        date = date + datetime.timedelta(days=-1)\n",
    "    elif date.isoweekday() == 7:\n",
    "        date = date + datetime.timedelta(days=-2)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "1. Pubication Date\n",
    "2. Publication Category\n",
    "3. Recent Movements (Normalized with S&P 500 Movements)<br>\n",
    "    a. 1 Day<br>\n",
    "    b. 1 Month (5 Day MA)<br>\n",
    "    c. 1 Quarter (10 Day MA)<br>\n",
    "    d. 1 Year(20 Day MA)<br>\n",
    "4. VIX at publication release\n",
    "5. Publicaion Corpus<br>\n",
    "    a. Unigram<br>\n",
    "    b. NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus, dt = extract_text(ebay_df['txt_link'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'month_before': 37.74,\n",
       " 'quarter_before': 37.64,\n",
       " 'week_before': 39.82,\n",
       " 'year_before': 31.83}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closing_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Current S&P 500 List, Stock Ticker, and CIK\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "cik_df = pd.read_html(wiki_url,header=[0],index_col=0,attrs={\"class\":\"wikitable\"})[0]\n",
    "cik_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "company_list = cik_df.todict()\n",
    "for company in company_list.keys():\n",
    "    df_list = df_list.append(get_sec_docs(company_list[company]))\n",
    "crawled_df = pd.concat(df_list,axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
